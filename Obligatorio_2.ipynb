{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ianarazny/Laboratorio-Arquitectura-en-Computadoras/blob/main/Obligatorio_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import numpy.linalg as la\n",
        "import numpy.random as rnd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "_pgw99tmv1jD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnd.seed(2023)\n",
        "n= 200\n",
        "m= 150 \n",
        "\n",
        "D = np.diag(1+100*rnd.rand(n))\n",
        "Q = D\n",
        "\n",
        "Qr,R = la.qr(rnd.rand(n,n))\n",
        "A = (Qr.T@D@Qr)[:,:m]\n",
        "b = rnd.rand(n)\n",
        "\n",
        "def f1(x):\n",
        "  return 0.5*la.norm(A@x-b)**2\n",
        "\n",
        "def grad_f1(x):\n",
        "  trans = A.T;\n",
        "  return trans@((A@x)-b);\n",
        "\n",
        "xstar_f1 = la.inv(A.T@A)@A.T@b"
      ],
      "metadata": {
        "id": "8PXrvWcK4haL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rosenbrock(x, b=2):\n",
        "    x1, x2 = x\n",
        "    return (1 - x1)**2 + b * (x2 - x1**2)**2\n",
        "\n",
        "def grad_rosenbrock(x, b=2):\n",
        "    x1, x2 = x\n",
        "    return np.array([\n",
        "        2 * (x1 - 1) - 4 * b * x1 * (x2 - x1**2),\n",
        "        2 * b * (x2 - x1**2)\n",
        "    ])\n",
        "\n",
        "xstar_rosenbrock = np.array([1,1])    "
      ],
      "metadata": {
        "id": "PyFTpatnxG4H"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuIE03Wqv05H"
      },
      "outputs": [],
      "source": [
        "# Parte 2.a)\n",
        "def gradient_descent(grad, x_init, xstar, alpha, tol=1e-5):\n",
        "    # Implementar el método de descenso por gradiente con paso fijo alpha, comenzando en x_init.\n",
        "    # grad es la función que implementa el gradiente de la función a optimizar, y xstar el óptimo conocido (esto en general no lo tenemos, pero para las funciones de este obligatorio sabemos los míniomos)\n",
        "    # La condición de parada debe ser que la norma del gradiente sea menor a la tolerancia establecida (tol)\n",
        "    # La función debe devolver el punto x alcanzado, la cantidad de iteraciones, un array con toda la trayectoria de los x_k, y un array con las distancias de los x_k al xstar. \n",
        "    \n",
        "    # Para devolver la trayectoria y distancias al óptimo, puede crear listas e ir anexando elementos a medida que transcurren las iteraciones.\n",
        "    # Las siguientes líneas sirven de ejemplo para esto.\n",
        "    xs = []\n",
        "    es = []\n",
        "    xs.append(x_init)\n",
        "    return x, it, np.array(xs), np.array(es)\n",
        "\n",
        "# Parte 2.b)\n",
        "def nesterov_gradient_descent(grad, x_init, xstar, alpha, tol=1e-5):\n",
        "    # Implementar el método de Nesterov por gradiente con paso fijo, comenzando en x_init\n",
        "    # La condición de parada debe ser que la norma del gradiente sea menor a la tolerancia establecida (tol)\n",
        "    # La función debe devolver el punto x alcanzado, la cantidad de iteraciones, un array con toda la trayectoria de los x_k, y un array con las distancias de los x_k al xstar. \n",
        "    return x, it, np.array(xs), np.array(es)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Código inicial para comenzar la parte 2.c). Cambiar el valor del paso alpha.\n",
        "x_init = np.array([1.5,2])\n",
        "x_gd, it_gd, xs_gd, e_gd = gradient_descent(grad_rosenbrock, x_init, xstar_rosenbrock, alpha=0)\n",
        "print('Gradient Descent')\n",
        "print('x =', x_gd)\n",
        "print('f(x) =', rosenbrock(x_gd))\n",
        "print('||grad f(x)|| =', la.norm(grad_rosenbrock(x_gd)))\n",
        "print('Iteraciones =', it_gd)"
      ],
      "metadata": {
        "id": "Uzku30Tew7Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Para la función ||Ax-b||^2, comenzar desde un punto aleatorio\n",
        "x_init = rnd.rand(m)"
      ],
      "metadata": {
        "id": "sO9cazMS6kKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parte 2.d)\n",
        "def gradient_descent_paso_decreciente(grad, x_init, xstar, tol=1e-5):\n",
        "    # Implementar el método de descenso por gradiente con paso decreciente, comenzando en x_init.\n",
        "    # Consejo: revise qué tiene que cumplir el paso decreciente. Si tiene problemas de velocidad de convergencia, modifique la velocidad de caída teniendo en cuenta los requisitos vistos en clase.\n",
        "    # La condición de parada debe ser que la norma del gradiente sea menor a la tolerancia establecida (tol)\n",
        "    # La función debe devolver el punto x alcanzado, la cantidad de iteraciones, un array con toda la trayectoria de los x_k, y un array con las distancias de los x_k al xstar. \n",
        "    return x, it, np.array(xs), np.array(es)"
      ],
      "metadata": {
        "id": "KX7zUbL5cBQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parte 2.e)\n",
        "def rosenbrock_diag(x, a=1, b=2):\n",
        "    # Completar\n",
        "    # Debe devolver la matriz diagonal para pre-multiplicar el gradiente, según el método de diagonal scaling, para la función de Rosenbrock\n",
        "    return \n",
        "\n",
        "def gradient_descent_diagonal_scaling(grad, diag_scaling, x_init, xstar, alpha, tol=1e-5):\n",
        "    # Implementar gradient descent con diagonal scaling y paso fijo alpha.\n",
        "    # La condición de parada debe ser que la norma del gradiente sea menor a la tolerancia establecida (tol)\n",
        "    # La función debe devolver el punto x alcanzado, la cantidad de iteraciones, un array con toda la trayectoria de los x_k, y un array con las distancias de los x_k al xstar. \n",
        "    return x, it, np.array(xs), np.array(es)"
      ],
      "metadata": {
        "id": "4Hqf8l0OdI2f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}